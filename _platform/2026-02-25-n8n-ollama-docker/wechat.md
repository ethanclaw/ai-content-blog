# 每个月烧几百块调用 AI API 值不值？

凌晨三点，手机突然震动。

云服务商发来告警：API 调用费用超出本月预算 300%。 才过去 10 天。

这已经不是第一次了。每次业务量增长，伴随而来的不是喜悦，而是对账单的恐惧。

数据隐私、不可控的延迟、随时变动的定价策略…这些问题像达摩克利斯之剑悬在头顶。

那么问题来了：是否有更好的选择？

---

## 这背后的本质是什么？

可以看到，云 AI 服务有三个根本问题：

**成本不可控**。按调用次数计费，业务量增长意味着成本线性增长。

**数据不可控**。请求数据需要离开本地网络，敏感场景无法使用。

**延迟不可控**。网络波动直接影响响应速度。

本质上，这是一个"依赖外部服务"的架构问题。解决思路很直接：把 AI 能力部署到本地。

---

## 为什么 n8n + Ollama 是合理的选择？

可以从四个维度来看：

### 1. 成本

| 方案 | 成本结构 |
|------|----------|
| 云服务 | $0.002-$12/千token，月付 |
| 本地部署 | 一次性硬件投入，后续 0 成本 |

月调用量超 100 万 token 后，本地方案的性价比开始显现。

### 2. 延迟

网络延迟通常是 50-500ms，而本地推理可以做到 10-100ms。

对于需要实时交互的场景，这是一个数量级的提升。

### 3. 隐私

数据永远不离开本地服务器。这对于处理敏感信息的场景尤为重要。

### 4. 灵活性

可以运行任意开源模型，根据任务选择最合适的模型，而不是被提供商绑定。

---

## 架构设计的核心逻辑

可以看到，整体架构有三个核心组件：

**n8n** 负责自动化工作流编排——触发器、节点、数据处理。

**Ollama** 负责 AI 推理——本地加载和运行大语言模型。

**Docker** 提供环境隔离和便捷部署。

那么，这三者如何协同？

本质上，n8n 作为"调度层"调用 Ollama 作为"执行层"，Docker 作为"基础设施层"提供运行环境。

---

## 部署的核心要点

### 1. Ollama

```bash
docker run -d --name ollama \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama:latest
```

关键是映射 11434 端口，并持久化模型数据。

### 2. n8n

```yaml
extra_hosts:
  - "host.docker.internal:host-gateway"
```

这个配置让容器内的 n8n 可以访问宿主机上的 Ollama。

### 3. 常用模型

| 模型 | 显存 | 特点 |
|------|------|------|
| Llama 3.1 8B | 8GB | 平衡性能 |
| Qwen 2.5 7B | 8GB | 中文能力强 |
| Mistral 7B | 8GB | 推理速度快 |

---

## 一个思考框架

回到最初的问题：为什么要自托管？

可以看到，这不仅仅是技术选择，更是一种"掌控权"的回归：

- 掌控成本结构
- 掌控数据流向
- 掌控响应速度
- 掌控模型选择

当你掌控了自己的 AI 基础设施，你就拥有了定义智能化未来的主动权。

---

*动手试试看吧！*
